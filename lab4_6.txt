[pyspark, <, sample.py]
[pyspark, --packages, org.apache.spark.sql, <, sample.py]


    external_links:
      - mongodb

sudo zip -r lsdp_4_5.zip project_1

1) Why Java: spark is written in Java due to complexity optimization
2) Java 8 needed?: Yes, Spark runs on Java 8+
3) External cluster connection available?: 
4) Code deployment to Spark cluster?: 
5) Spark jobs observation:
6) Logistic regression --> non-linear regression
7) Multi-class: many classes and only one could be assigned; Multi-label: many classes and many could be assigned
8) RDD - Resilent Distributed Dataset: immutable dataset which can be multiprocessed; transformations & actions (parallelize)
9) Dataframe - mutable table-looking data structure
10) DataSet - datasets created from sql query
11) Master: SparkContext; Worker: Pipelines



Kubernetes:

Kubernetes Pod may run many containers.

Deployment: deploying or rolling back deployment (file of deployment)
Service: pod or many pods (many replicas)
DeamonSet: ensurement about some nodes run a copy of a Pod; 
StatefulSet: workload API used to manage stateful applications; guarentees about ordering and uniqueness of Pods (set of pods)
[StatefulSet --> stateful; DeamonSet --> unstateful]
Configmap: assignment of configuration to pods
Secret: security of sensitive files i.e. passwords
Persistent volume (claim): user's storage request; specific size, volume mode, access mode, resources 

Creation order: service --> configmap --> deployment
Deployment always create at the end. Deployment creates pod.
Service --> pod backend
Configmap --> pod frontend

kubectl create -f deployment.yaml --> create deployment
kubectl apply -f deployment.yaml --> update (use) deployment
kubectl delete -f deployment.yaml --> delete deployment

Debugging:
kubectl logs --previous sample-pod
kubectl describe pod sample-pod
